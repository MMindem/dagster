---
title: IO Managers
description: IO managers are user-provided objects that store solid outputs and load them as inputs to downstream solids.
---

# IO Managers

IO managers are user-provided objects that store solid outputs and load them as inputs to downstream solids.

<img src="/images/io-managers.png" />

## Relevant APIs

| Name                                                                        | Description                                                                                                                            |
| --------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| <PyObject module="dagster" object="io_manager" displayText="@io_manager" /> | The decorated function used to define IO managers.                                                                                     |
| <PyObject module="dagster" object="IOManager" />                            | Base class for user-provided IO managers.                                                                                              |
| <PyObject module="dagster" object="OutputDefinition" />                     | Defines an output from a solid’s compute function. More information can be found on the [Inputs and Outputs](/inputs-and-outputs) page |
| <PyObject module="dagster" object="ModeDefinition" />                       | Defines a mode in which a pipeline can operate. More information can be found on the [Modes](/concepts/modes) page                     |

## Overview

Dagster solids have [inputs and outputs](/overview/solids-pipelines/solids#solid-inputs-and-outputs). When a solid produces an output, what happens to it? When a solid needs an input, how is it loaded?

<PyObject module="dagster" object="IOManager" />s let users decide how solid
outputs are stored and loaded in downstream solids.
<PyObject module="dagster" object="DagsterTypeLoader" />s and
<PyObject module="dagster" object="RootInputManager" />s (experimental) let
users decide how inputs at the beginning of a pipeline are loaded.

These APIs make it easy separate code that's responsible for logical data transformation
from code that's responsible for reading and writing the results. Solids can focus
on business logic, while IO managers handle I/O. This separation makes it easier
to test the business logic and run it in different environments.

---

## Defining an IO manager

To define a IO manager, use the <PyObject module="dagster" object="io_manager" displayText="@io_manager" /> decorator.

```python
class MyIOManager(IOManager):
    def handle_output(self, context, obj):
        write_csv("some/path")

    def load_input(self, context):
        return read_csv("some/path")

@io_manager
def my_io_manager(init_context):
    return MyIOManager()
```

See more details on building a custom IO manager in [Custom IO Managers](/concepts/io-management/custom-io-managers).

### Outputs and downstream inputs

<PyObject module="dagster" object="IOManager" />s are user-provided objects that
are responsible for storing the output of a solid and loading it as input to
downstream solids. For example, an IOManager might store and load objects from
files on a filesystem. Each solid output can have its own IOManager, or multiple
solid outputs can share an IOManager. The IOManager that's used for handling a
particular solid output is automatically used for loading it in downstream
solids.

<p />

I.e. an IOManager handles the teal boxes:

<p align="center">
  <img src="/assets/images/concepts/io-managers.png" />
</p>

The default IOManager, <PyObject module="dagster" object="mem_io_manager" />, stores outputs in memory, but this only works for the single process executor. Dagster provides out-of-the-box IOManagers that pickle objects and save them. These are <PyObject module="dagster" object="fs_io_manager"/>, <PyObject module="dagster_aws.s3" object="s3_pickle_io_manager"/>, <PyObject module="dagster_azure.adls2" object="adls2_pickle_io_manager"/>, or <PyObject module="dagster_gcp.gcs" object="gcs_pickle_io_manager"/>.

IOManagers are [resources](/overview/modes-resources-presets/modes-resources), which means users can supply different IOManagers for the same solid outputs in different situations. For example, you might use an in-memory IOManager for unit-testing a pipeline and an S3IOManager in production.

---

## Using an IO manager

### Pipeline-wide IO manager

By default, all the inputs and outputs in a pipeline use the same IOManager. This IOManager is determined by the <PyObject module="dagster" object="ResourceDefinition" /> provided for the `"io_manager"` resource key. `"io_manager"` is a resource key that Dagster reserves specifically for this purpose.

Here’s how to specify that all solid outputs are stored using the <PyObject module="dagster" object="fs_io_manager" />, which pickles outputs and stores them on the local filesystem. It stores files in a directory with the run ID in the path, so that outputs from prior runs will never be overwritten.

<!-- ```python literalinclude caption=default_io_manager.py
file:/docs_snippets/docs_snippets/overview/io_managers/default_io_manager.py
``` -->

```python
@pipeline(mode_defs=[ModeDefinition(resource_defs={"io_manager": fs_io_manager})])
def my_pipeline():
    solid2(solid1())
```

### Per-output IO manager

Not all the outputs in a pipeline should necessarily be stored the same way. Maybe some of the outputs should live on the filesystem so they can be inspected, and others can be transiently stored in memory.

To select the IOManager for a particular output, you can set an `io_manager_key` on the <PyObject module="dagster" object="OutputDefinition" />, and then refer to that `io_manager_key` when setting IO managers in your <PyObject module="dagster" object="ModeDefinition" />. In this example, the output of solid1 will go to `fs_io_manager` and the output of solid2 will go to `mem_io_manager`.

<!-- ```python literalinclude caption=io_manager_per_output.py
file:/docs_snippets/docs_snippets/overview/io_managers/io_manager_per_output.py
startAfter:start_marker
endBefore:end_marker
``` -->

```python
@solid(output_defs=[OutputDefinition(io_manager_key="fs")])
def solid1(_):
    return 1


@solid(output_defs=[OutputDefinition(io_manager_key="mem")])
def solid2(_, a):
    return a + 1


@pipeline(mode_defs=[ModeDefinition(resource_defs={"fs": fs_io_manager, "mem": mem_io_manager})])
def my_pipeline():
    solid2(solid1())
```

---

## Examples

### With per-output config

When launching a run, you might want to parameterize how particular outputs are stored.

For example, if your pipeline produces DataFrames to populate tables in a data warehouse, you might want to specify the table that each output goes to at run launch time.

To accomplish this, you can define an `output_config_schema` on the IO manager definition. The IOManager methods can access this config when storing or loading data, via the <PyObject module="dagster" object="OutputContext" />.

```python literalinclude caption=output_config.py
file:/docs_snippets/docs_snippets/overview/io_managers/output_config.py
startAfter:io_manager_start_marker
endBefore:io_manager_end_marker
```

Then, when executing a pipeline, you can pass in this per-output config.

<!-- ```python literalinclude caption=output_config.py
file:/docs_snippets/docs_snippets/overview/io_managers/output_config.py
startAfter:execute_start_marker
endBefore:execute_end_marker
``` -->

```python
class MyIOManager(IOManager):
    def handle_output(self, context, obj):
        table_name = context.config["table"]
        write_dataframe_to_table(name=table_name, dataframe=obj)

    def load_input(self, context):
        table_name = context.upstream_output.config["table"]
        return read_dataframe_from_table(name=table_name)


@io_manager(output_config_schema={"table": str})
def my_io_manager(_):
    return MyIOManager()
```

### With per-output metadata (experimental)

You might want to provide static metadata that controls how particular outputs are stored. You don't plan to change the metadata at runtime, so it makes more sense to attach it to a definition rather than expose it as a configuration option.

For example, if your pipeline produces DataFrames to populate tables in a data warehouse, you might want to specify that each output always goes to a particular table. To accomplish this, you can define `metadata` on each <PyObject module="dagster" object="OutputDefinition" />:

<!-- ```python literalinclude caption=metadata.py
file:/docs_snippets/docs_snippets/overview/io_managers/metadata.py
startAfter:solids_start_marker
endBefore:solids_end_marker
``` -->

```python
@solid(output_defs=[OutputDefinition(metadata={"schema": "some_schema", "table": "some_table"})])
def solid1(_):
    """Return a Pandas DataFrame"""


@solid(output_defs=[OutputDefinition(metadata={"schema": "other_schema", "table": "other_table"})])
def solid2(_, _input_dataframe):
    """Return a Pandas DataFrame"""
```

The IOManager can then access this metadata when storing or retrieving data, via the <PyObject module="dagster" object="OutputContext" />.

In this case, the table names are encoded in the pipeline definition. If, instead, you want to be able to set them at run time, the next section describes how.

<!--
```python literalinclude caption=metadata.py
file:/docs_snippets/docs_snippets/overview/io_managers/metadata.py
startAfter:io_manager_start_marker
endBefore:io_manager_end_marker
``` -->

```python
class MyIOManager(IOManager):
    def handle_output(self, context, obj):
        table_name = context.metadata["table"]
        schema = context.metadata["schema"]
        write_dataframe_to_table(name=table_name, schema=schema, dataframe=obj)

    def load_input(self, context):
        table_name = context.upstream_output.metadata["table"]
        schema = context.upstream_output.metadata["schema"]
        return read_dataframe_from_table(name=table_name, schema=schema)


@io_manager
def my_io_manager(_):
    return MyIOManager()

```

---

## Pattern

### Root input manager (experimental)

When you have a solid at the beginning of a pipeline that operates on data from external source, you might wish to separate that I/O from your solid's business logic, in the same way you would with an IO manager if the solid were loading from an upstream output.

To accomplish this, you can define an <PyObject module="dagster" object="RootInputManager" />.

<!-- ```python literalinclude caption=root_input_manager.py
file:/docs_snippets/docs_snippets/overview/io_managers/root_input_manager.py
startAfter:start_marker
endBefore:end_marker
``` -->

```python
@solid(input_defs=[InputDefinition("dataframe", root_manager_key="my_root_manager")])
def my_solid(_, dataframe):
    """Do some stuff"""


@root_input_manager
def table1_loader(_):
    return read_dataframe_from_table(name="table1")


@pipeline(mode_defs=[ModeDefinition(resource_defs={"my_root_manager": table1_loader})])
def my_pipeline():
    my_solid()
```

Setting the `root_manager_key` on an `InputDefinition` controls how that input is loaded in pipelines where it has no upstream output.

The <PyObject module="dagster" object="root_input_manager" /> decorator behaves nearly identically to the <PyObject module="dagster" object="resource" /> decorator. It yields an <PyObject module="dagster" object="RootInputManagerDefinition" />, which is a <PyObject module="dagster" object="ResourceDefinition" /> that will produce an <PyObject module="dagster" object="RootInputManager" />.

#### Per-input config

When launching a run, you might want to parameterize how particular root inputs are loaded.

To accomplish this, you can define an `input_config_schema` on the input manager definition. The load function can access this config when storing or loading data, via the <PyObject module="dagster" object="InputContext" />.

<!-- ```python literalinclude caption=config_input_manager.py
file:/docs_snippets/docs_snippets/overview/io_managers/config_input_manager.py
startAfter:def_start_marker
endBefore:def_end_marker
``` -->

```python
@root_input_manager(input_config_schema={"table_name": str})
def table_loader(context):
    return read_dataframe_from_table(name=context.config["table_name"])

@pipeline(mode_defs=[ModeDefinition(resource_defs={"my_root_manager": table_loader})])
def my_pipeline():
    my_solid()
```

Then, when executing a pipeline, you can pass in this per-input config.

<!-- ```python literalinclude caption=config_input_manager.py
file:/docs_snippets/docs_snippets/overview/io_managers/config_input_manager.py
startAfter:execute_start_marker
endBefore:execute_end_marker
``` -->

```python
execute_pipeline(
    my_pipeline,
    run_config={"solids": {"my_solid": {"inputs": {"dataframe": {"table_name": "table1"}}}}},
)
```

#### With subselection

You might want to execute a subset of solids in your pipeline and control how the inputs of those solids are loaded. Root input managers also help in these situations, because the inputs at the beginning of the subset become the new "roots".

For example, you might have `solid1` that normally produces a table that `solid2` consumes. To debug `solid2`, you might want to run it on a different table than the one normally produced by `solid1`.

To accomplish this, you can set up the `root_manager_key` on `solid2`'s `InputDefinition` to point to an input manager with the desired loading behavior. As before, setting the `root_manager_key` on an `InputDefinition` controls how that input is loaded when it has no upstream output.

<!-- ```python literalinclude caption=subselection.py
file:/docs_snippets/docs_snippets/overview/io_managers/subselection.py
startAfter:start_marker
endBefore:end_marker
``` -->

```python
@root_input_manager(input_config_schema={"table_name": str})
def my_root_input_manager(context):
    return read_dataframe_from_table(name=context.config["table_name"])


class MyIOManager(IOManager):
    def handle_output(self, context, obj):
        table_name = context.name
        write_dataframe_to_table(name=table_name, dataframe=obj)

    def load_input(self, context):
        return read_dataframe_from_table(name=context.upstream_output.name)


@io_manager
def my_io_manager(_):
    return MyIOManager()


@solid(output_defs=[OutputDefinition(io_manager_key="my_io_manager")])
def solid1(_):
    """Do stuff"""


@solid(input_defs=[InputDefinition("dataframe", root_manager_key="my_root_input_manager")])
def solid2(_, dataframe):
    """Do stuff"""


@pipeline(
    mode_defs=[
        ModeDefinition(
            resource_defs={
                "my_io_manager": my_io_manager,
                "my_root_input_manager": my_root_input_manager,
            }
        )
    ]
)
def my_pipeline():
    solid2(solid1())

```

When running the full pipeline, `solid2`'s input will be loaded using the IO manager on the output of `solid1`. When running the pipeline subset, `solid2`'s input has no upstream output, so the input manager corresponding to its `root_manager_key` is used.

<!-- ```python literalinclude caption=subselection.py
file:/docs_snippets/docs_snippets/overview/io_managers/subselection.py
startAfter:start_execute_subselection
endBefore:end_execute_subselection
``` -->

```python
execute_pipeline(
    my_pipeline,
    solid_selection=["solid2"],
    run_config={"solids": {"solid2": {"inputs": {"dataframe": {"table_name": "tableX"}}}}},
)
```
